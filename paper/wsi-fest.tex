\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{url}
%\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsfonts,amsthm,amstext,amscd}
\usepackage{color}
\newcommand{\jiweil}{\texttt{jiweil}}
\newcommand{\huang}{\texttt{huang}}
\newcommand{\neelakantan}{\texttt{neela}}
\newcommand{\Ro}{\mathbb{R}^{d_1}}
\newcommand{\Rt}{\mathbb{R}^{d_2}}
\usepackage{tikz}
\usepackage{booktabs}
%\usepackage{multirow,multicol}
\usepackage{cleveref}
%\usepackage{graphicx}
%\usepackage{dblfloatfix}
%\usepackage{array}
%\usepackage{changepage}

\hyphenation{poly-semy}
%\usepackage{dcolumn}
%\newcolumntype{d}[1]{D{.}{.}{#1}}

%\usepackage{todonotes}

\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{28} %  Enter the acl Paper ID here

\setlength\titlebox{8cm}
%\setlength\titlebox{50ex}  % Vertical space allocated for \Authors 
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Cross-lingual word sense induction with multi-sense word embeddings}
\author{
  Márton Makrai \\
  %Institute for Linguistics\\ 
  %Hungarian Academy of Sciences \\
  %Bencz\'ur u. 33 \\ 1068 Budapest, Hungary \\
  %{\tt makrai.marton@nytud.mta.hu} \\
}
\date{}

\begin{document}
\maketitle

\hspace{2cm}

%\begin{abstract}
%\end{abstract}

\emph{Word sense induction} (WSI) is the task of discovering senses of words
without supervision \citep{Schutze:1998}. Recent approaches include multi-sense
word embeddings (MSEs), vector space models of word distribution with more
vectors for ambiguous words, each vector corresponding to some word sense.
\cite{Borbely:2016} proposed a cross-lingual method for the evaluation of MSEs
exploiting the principle that words may be ambiguous as far as the postulated
senses translate to different words in some other language. We follow
\cite{Xing:2015} in formulating transformation as an orthogonal mapping from
the source language embedding to the target one, trained on a seed of a few
thousand word pairs. In the multi-sense setup, we perform such translations
from MSEs, with the idea that what are different senses in the source language
will very often translate to different words in the target language.

\url{https://github.com/makrai/wsi-fest}

\section{Monosemy: a less delicious sense inventory}

The differentiation of word senses, as already noted in \cite{Borbely:2016}, is
fraught with difficulties, especially when we wish to distinguish homophony,
using the same written or spoken form to express different concepts, such as
Russian {\it mir} `world' and {\it mir} `peace' from polysemy, where speakers
feel that the two senses are very strongly connected, such as in Hungarian {\it
nap} `day' and {\it nap} `sun'.  To quote \cite{Zgusta:1971} ``Of course it is
a pity that we have to rely on the subjective interpretations of the speakers,
but we have hardly anything else on hand''. Etymology makes clear that
different languages make different lump/split decisions in the conceptual
space, so much so that translational relatedness can, to a remarkable extent,
be used to recover the universal clustering \citep{Youn:2016}.

\subsection{Lexical ambiguity (Veronika Lipp)}

Lexical ambiguity is linguistically subdivided into two main categories:
\emph{homonymy and polysemy} \citep{Cruse:2004}. Homonymous words have semantically
unrelated and mutually incompatible meanings, such as \emph{punch$_1$} , which
means `a blow with a fist', and \emph{punch$_2$}, which means `a drink'. Some
have described such homonymous word meanings as essentially distinct words that
accidentally have the same phonology (e.g., Murphy, 2002). Polysemous words, on
the other hand, have semantically related or overlapping senses (\cite{Cruse:2004,Jackendoff:02, Pustejovsky:19995}), such as \emph{mouth} meaning both `organ
of body' and `entrance of cave'.

Two criteria have been proposed for the distinction between homonymy and
polysemy. The first criterion has to do with the \emph{etymological} derivation
of words. Words that are historically derived from distinct lexical items are
taken to be homonymous. However, the etymological criterion is not always
decisive. One reason is that there are many words whose historical derivation
is uncertain. Another reason is that it is not always very clear how far back
we should go in tracing the history of words \citep{Lyons:1977}.  

The second criterion for the distinction between homonymy and polysemy has to
do with the \emph{relatedness/unrelatedness of meaning}.  The distinction
between homonymy and polysemy seems to correlate with the native speaker’s
feeling that certain meanings are connected and that others are not. Generally,
unrelatedness in meaning points to homonymy, whereas relatedness in meaning
points to polysemy. 
% Marci: ez utóbbi mondat fontos a projekt szempontjából. Ha húzunk, akkor
% inkább a %teljes etymological bekezdést hagyjuk el.
% However, it seems that ``relatedness of meaning'' is not an all-or-nothing
% relation, but rather a matter of degree.
However, in a large number of cases, there does not seem to be an agreement among native
speakers as to whether the meanings of the words are related. So, it seems that
there is not a clear dichotomy between homonymy and polysemy, but rather a
continuum from ‘‘pure’’ homonymy to ‘‘pure’’ polysemy \citep{Lyons:1977}.

Most discussions about lexical ambiguity, within theoretical and computational
linguistics, concentrate on polysemy, which can be further divided into two
types (c.f. \cite{apr:1974} and \cite{pus:19995}). The first type of polysemy is motivated by
\emph{metaphor (irregular polysemy)}. In metaphorical polysemy, a relation of analogy is assumed to
hold between the senses of the word. The basic sense of metaphorical polysemy
is literal, whereas its secondary sense is figurative. For example, the
ambiguous word \emph{eye} has the literal basic sense  `organ of the body' and
the figurative secondary sense `hole in a needle.' The other type of polysemy
is motivated by \emph{metonymy (regular polysemy)}. In metonymy, the relation that is assumed to
hold between the senses of the word is that of contiguity or connectedness.
%Apresjan argued that metonymically motivated polysemy respects the usual
%notion of polysemy, which is the ability of a word to have several distinct
%but related meanings. 
In metonymic polysemy, both the basic and the secondary senses are literal. For
example, the ambiguous word \emph{chicken} has the literal basic sense
referring to the animal and the literal secondary sense of the meat of that
animal. 

% It is customary in the literature to distinguish between regular or logical
% polysemy, on the one hand, and irregular or accidental polysemy, on the other
% (Apresjan, 1974; Pustejovsky, 1995). 

``After centuries of practical lexicography, there is still hardly any
consensus on how to divide the semantic space of a lexical item''
\citep{mee:2006}. This is reflected in the fact that dictionaries, even those for
the same types of users and of similar size, offer (very) different sense
division of many words. As \citet{moo:1987} says, there is no absolute answer on
how many senses a word has and in which order they should be presented. Senses
of polysemous words can be described at different levels of granularity, either
more towards broader categories (lumping) of fine-grained ones (splitting),
depending on the purpose of the dictionary, its size, target users etc.

One of the problems of dictionaries is their approach to meaning description,
i.e.~presenting words in isolation using the numbered lists of definitions
%(which are actually a metalexicographic construct), fails to reflect actual
language use \citep{han:2000}. Sue Atkins, a well-known lexicographer, has
famously said that she does not believe in word senses \citep{kil:19997}.
Similarly, researchers in translation theory (\cite{nid:19997}) have argued that
words have no meaning without context. Context is the key for describing word
meanings and disambiguating between them, an idea that goes back to Firth and
his famous quote ``You shall know by the company it keeps'' \citep{Firth:1957}.
Another theory that is based on this view is lexical
priming by \citet{hoe:2005} whose research shows that collocations, semantic
associations and colligations of a word play a decisive role in differentiating
between its senses.
%In relation to this, it is particularly important to mention Hanks' theory of Norms and Exploitations (Hanks 2013), which has at its core the idea that isolated words do not have meanings, but rather meaning potentials. According to Hanks, word meanings do exist, ``but traditional descriptions are misleading'' (ibid.: 82). Hanks goes on to suggest that a major future task in addressing meaning ``will be to identify meaning components, the ways in which they combine, relations with the meaning components of semantically related words, and the phraseological circumstances in which they are activated'' (ibid.: 82). This is highly relevant not only for lexicography, but also for linguistics in general, as well as other disciplines such as natural language processing.
Word ambiguity may pose great problems in language understanding both by humans and computers.
%T may pose great problems of 
%T legyünk szerényebbek: önamgában az hogy a szavaknak több jelentésük van nem feltétlenül okoz problémát a beszélőknek. A megfelelő kontextusban ugyanis fel sem tűnik nekik, hogy más jelentései is vannak a szavaknak, amelyeket használnak. 
%The underlying reason for this is twofold: first, lexicons may comprise overlapping meanings, which render the unique assignment of the right meaning to the given word impossible even for human annotators (above chance assignment). Secondly, the granularity of senses in the lexicon may differ, which, to the extreme, may result in one sense being completely subsumed under the other.
As discussed above, homonymy, metonymy and polysemy have fuzzy borders 
%where co-occurrence statistics and cross-lingual comparison provide two independent sources of information. 
therefore, a methodology is needed that is able to clear-cut the fuzzy border of overlapping meanings and is able to find the right grain-size for the specific task.

%Moreover, listing as many senses of an ambiguous word (or frames attaching to a verb) as we can, misses the information that some meanings (and frames) are related. It is standard to distinguish \emph{homonymy}, arbitrary coincidence of word forms e.g.~Hungarian \emph{vár} `wait; castle', \emph{metonymy} (e.g.~\emph{school} and many other institution nouns are used to express both the building and the people involved), and  \emph{polysemy} (word forms with related but remarkably different senses e.g.~Hungarian \emph{nap} `day; Sun` or \emph{gerinc} `spine; ridge').
%E.:information for what. 
%The three categories have fuzzy borders where 
Co-occurrence statistics and cross-lingual comparison provide two independent sources of information.
\citet{Youn:2016} analyses the universal structure of lexical semantics, and show
that some concepts are more prone to polysemy than others, and that there are
clusters of concepts within which polysemy is significantly more frequent
than across the clusters. 
% While homonyms are usually considered to be non-overlapping semantic fields that easily lend themselves to sense-tagging, polysemic meanings pose real challenge for sense-tagging as they are more likely to be overlapping entities.
\section{Multi-sense word embeddings}

MSEs originate with \cite{Reisinger:2010,Huang:2012} who use a uniform number
of clusters for all words that are selected as potentially ambiguous.  adaptive
sense numbers \citep{Neelakantan:2014}, new senses are introduced during
training.  Dirichlet process, AdaGram \citep{Bartunov:2015}, where senses may
be merged as well as allocated during training; and and mutli-sense skip-gram
\citep{Li:2015}
% mi a fő különbség Reisinger és Huang között?
MSEs are yet in research phase, as \cite{Li:2015}  demonstrate that, when
meta-parameters are carefully controlled for, MSEs introduce a slight
performance boost in semantics-related tasks (semantic similarity for words and
sentences, semantic relation identification, part-of-speech tagging), but
similar improvements can be achieved by increasing the dimension of a
single-sense embedding.

semantic resolution

\section{Linear translation}

 \cite{Mikolov:2013x} discovered that embeddings of different languages are so
 similar that a linear transformation can map vectors of the source language
 words to the vectors of their translations.  

\begin{figure}[b]
    \centering
    \resizebox{\columnwidth}{!} {
        \begin{tikzpicture}[line width=2pt] %[&gt;=stealth']
            \tikzstyle{every node}=[font=\huge]
            %\tikzset{every edge thick}
            \node          (hf) at (2, 2)  {finom};
            \node          (hf) at (1.7, 1.4)  {értelmezés};
            \node          (hs) at (5, 4)  {finom};
            \node          (hs) at (4.5, 4.6)  {tanulmány};
            \node          (gf) at (11, 2) {fine};
            \node          (gf) at (11, 1.4) {interpretation};
            \node		 (gs) at (14, 4) {delicions};
            \node		 (gs) at (15.5, 3.4) {memorandum };
            \node (ho) at (0, 0) {};
            \node (hx) at (8, 0) {};
            \node (hy) at (0, 6) {};
            \node (go) at (9,0) {};
            \node (gx) at (17,0) {};
            \node (gy) at (9,6) {};

            \draw[->] (ho) edge (hx);
            \draw[->] (ho) edge (hy);
            \draw[->] (go) edge (gx);
            \draw[->] (go) edge (gy);
        \end{tikzpicture}
    }
    \caption{Linear translation of word senses. The Hungarian word
        \emph{finom} is ambiguous between `fine' and `delicions'.  The two
        senses are identified by the ``neighboring'' words \emph{értelmezés}
    `interpretation' and \emph{tanulmány} `memorandum'.
    } \label{fig:AdaGram}
\end{figure}

The method uses a seed dictionary of a few thousand words to learn translation
as a linear mapping $W: \mathbb{R}^{d_1}\rightarrow \mathbb{R}^{d_2}$ from the
source (monolingual) embedding to the target: the translation $z_i \in \Rt$ of
a source word $x_i \in \Ro$ is approximately its image $Wx_i$ by the mapping.
The translation model is trained with linear regression on the seed dictionary

\[\min_W \sum_i || Wx_i - z_i ||^2 \]

\noindent and can be used to collect translations for the whole vocabulary by
choosing $z_i$ to be the nearest neighbor of $Wx_i$.

% TODO We follow \cite{Mikolov:2013x} in using different metrics, Euclidean
% distance in training and cosine similarity in collection of translations.
% Though this choice is theoretically unmotivated, it seems to work better than
% more consistent use of metrics; but see \citep{Xing:2015} for opposing
% results.

A common error in linear translation is when there are \emph{hubs}, target
words returned as the translation of many words, which is wrong in most of the
cases.  \cite{Dinu:2015} propose a method they call \emph{global correction}
for decreasing the importance of such target words. Our experiments use this
method.
% Borbely:2016-ban nem használtuk a Dinu-nak a "global correction"-jét memória
% problémák miatt

In a multi-sense embedding scenario, we take a multi-sense embedding
as source model, and a single-sense embedding as target model.
We evaluate a specific source MSE model in two ways 
referred as \emph{single}, and \emph{multiple}.

The tools that generate MSEs all provide fall-backs to singe-sense embeddings
in the form of so called global vectors. The method \emph{single} can be
considered as a baseline; a traditional, single-sense translation between the
global vectors and the target vectors.  Note that the seed dictionary may
contain overlapping translation pairs: one word can have multiple translations
in the gold data, and more than one word can have the same translation.  In
the \emph{multiple} method we used the same translation matrix, trained on the
global vectors, and inspected the translations of the different senses of the
same source word.  Exploiting the multiple sense vectors one word can have
more than one translation.

\section{Data}

\section{Experiments}

restrict the vocabulary

Two evaluation metrics were considered, \emph{lax} and \emph{strict}. In lax
evaluation a translation is taken to be correct if any of the source word's
senses are translated into any of its gold translations.  In strict evaluation
the translations of the source word are expected to cover all of its gold
translations.  For example if {\it finom} has two gold translations, {\it
  delicions} and {\it fine}, and its actual translations are `delicions' and some
word other than `fine', then it has a lax score of $2$, but a strict score
of $1$.

The quality of the translation was measured by training on the most frequent 5k
word pairs and evaluating on another 1k seed pairs.  We used Witionary as our
seed dictionary, extracted with
\texttt{wikt2dict}\footnote{\url{https://github.com/juditacs/wikt2dict}}
\citep{Acs:2016}. 
% TODO shows the percentage of correctly translated words for {\bf
% s}ingle-sense and {\bf m}ulti-sense translation. 

``forward'' neighbor search: precision increases with vocabulary cut-off
(tryied up to $2^16=65536$


\section{Acknowledgments}

1957 was an influential year in linguistics: \cite{Harris:1957} developed the
frequency-aware variant of the distributional method, \cite{Osgood:1957}
pioneered vector space models, and the author of a more recent conceptual
meaning representation framework \citep{Kornai:2010,Kornai:2017} was born.
Fifty years later (more precisely in fall 2006) I learned András during a class
he taught on the book he was writing \citep{Kornai:2007}. I heard about
\emph{deep cases} and \emph{k\={a}rakas} sooner than I did about \emph{thematic
roles}. The fact that he taught me computational linguistics in a master and
disciple fashion comes at no surprise as there have been no such PhD school in
Hungary, not to mention mathematical linguistics proper.  
% He advised my so called \emph{topic labor} and MSc at the Budapest University
% of Technology and Economics, and 
I started my PhD as a member of the 4lang project at the Institute for Computer
Science and Control of the Hungarian Academy of Sciences. 

Laozi says that a good leader does not leave a footprint, and András encouraged
us to be independent and effective. One of his remarkable citations is that
``It's easier to ask forgiveness than it is to get permission''. The proverb is
sometimes attributed to the Jesuit, who are similar to András in having had a
great impact on what I've become in the past ten years. The real source of the
proverb is Grace Hopper, a computer scientist and US navy admiral who invented
the first compiler and popularized the idea of machine-independent programming
languages. Both the alleged and the real source remind me to that I still have
lessons to learn in efficiency.

% \cite{Swadesh:1955}

\bibliographystyle{acl_natbib} 
\bibliography{snn}  

\end{document}

%\section{Ten years of semi-supervised word sense induction} 
