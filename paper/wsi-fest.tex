\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{url}
%\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsfonts,amsthm,amstext,amscd}
%\usepackage[usenames, dvipsnames]{color}

%\newcommand{\huang}{\texttt{huang}}
\newcommand{\neelakantan}{\texttt{neela}}
\newcommand{\adagram}{\texttt{AdaGram}}
\newcommand{\mutli}{\texttt{mutli}}
\newcommand{\Ro}{\mathbb{R}^{d_1}}
\newcommand{\Rt}{\mathbb{R}^{d_2}}
\newcommand{\fl}{\texttt{4lang}}

\usepackage{tikz}
\definecolor{hltblue}{RGB}{3,61,92}
\definecolor{hltdarkgreen}{RGB}{26,148,129}
\definecolor{hltlightgreen}{RGB}{155,204,147}
\definecolor{hltyellow}{RGB}{252,238,166}

\usepackage{booktabs}
%\usepackage{multirow,multicol}
\usepackage{cleveref}
%\usepackage{graphicx}
%\usepackage{dblfloatfix}
%\usepackage{array}
%\usepackage{changepage}

\hyphenation{poly-semy}
%\usepackage{dcolumn}
%\newcolumntype{d}[1]{D{.}{.}{#1}}

\usepackage{todonotes}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{28} %  Enter the acl Paper ID here

%\setlength\titlebox{8cm}
%\setlength\titlebox{50ex}  % Vertical space allocated for \Authors
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Do multi-sense embeddings learn more senses? \\ An evalutaion in linear
translation}
\author{
  Márton Makrai
  %\\ Institute for Linguistics\\
  %Hungarian Academy of Sciences \\
  %Bencz\'ur u. 33 \\ 1068 Budapest, Hungary \\
  %{\tt makrai.marton@nytud.mta.hu} \\
}
\date{}


\begin{document}
\maketitle

%\hspace{2cm}

%\begin{abstract}
%\end{abstract}

\emph{Word sense induction} (WSI) is the task of discovering senses of words
without supervision \citep{Schutze:1998}. Recent approaches include multi-sense
word embeddings (MSEs), vector space models of word distribution with more
vectors for ambiguous words; each vector is supposed to corresponding to a different
word sense, but in parctice sense vectors seem to duplicate senses.

In \cite{Borbely:2016}, we proposed a cross-lingual method for the evaluation
of MSEs based on the principle that words may be ambiguous as far as the
postulated senses translate to different words in some other language.  We
applied the method by \citet{Mikolov:2013x} who train a translation mapping
from the source languages embedding to the target as a least-squares regression
supervised by a seed dictionary of the  few thousand most frequent words. The
translation of a source word vector is the nearest neighbor of its image by the
mapping in the target space. In the multi-sense setting, we translated from
MSEs (the target embedding remaining single-sense).

Now we develop the evaluation further. Part of the evaluation task is to decide
on empirical grounds whether different good translations of a word are synonyms
or translations in different senses.  We analyze sense vectors based on their
distance, which is not directly determined by the distance of the sense vectors
in the source space because of the nearest neighbor search. The reader should
be noted that the evaluation is not very strict, rather a process of
discovering something meaningful in available MSE implementations.

The code for these experiments can be found at
\url{https://github.com/makrai/wsi-fest}

\section{Towards a less \emph{delicious} inventory}

\begin{figure}[b]
    \centering
    \resizebox{\columnwidth}{!} {
        \begin{tikzpicture}[line width=2pt] %[&gt;=stealth']
            \tikzstyle{every node}=[font=\huge]
            %\tikzset{every edge thick}
            \node[text=hltdarkgreen] (hf) at (2, 2) {finom};
            \node (hf) at (1.7, 1.4)  {durva};
            \node[text=hltdarkgreen] (hs) at (5, 4)  {finom};
            \node (hs) at (4.5, 4.6)  {ízletes};
            \node[text=hltdarkgreen] (gf) at (11, 2) {fine};
            \node (gf) at (11, 1.4) {coarse};
            \node[text=hltdarkgreen] (gs) at (14, 4) {delicious};
            \node (gs) at (12.5, 4.6) {tasty};
            \node (ho) at (0, 0) {};
            \node (hx) at (8, 0) {};
            \node (hy) at (0, 6) {};
            \node (go) at (9,0) {};
            \node (gx) at (17,0) {};
            \node (gy) at (9,6) {};

            \draw[->] (ho) edge (hx);
            \draw[->] (ho) edge (hy);
            \draw[->] (go) edge (gx);
            \draw[->] (go) edge (gy);
        \end{tikzpicture}
    }
    \caption{Linear translation of word senses. The Hungarian word
        \emph{finom} is ambiguous between `fine' and `delicious'.}
        \label{fig:AdaGram}
\end{figure}

The differentiation of word senses, as already noted in \cite{Borbely:2016}, is
fraught with difficulties, especially when we wish to distinguish homophony,
using the same written or spoken form to express different concepts, such as
Russian {\it mir} `world' and {\it mir} `peace' from polysemy, where speakers
feel that the two senses are very strongly connected, such as in Hungarian {\it
nap} `day' and {\it nap} `sun'.  To quote \cite{Zgusta:1971} ``Of course it is
a pity that we have to rely on the subjective interpretations of the speakers,
but we have hardly anything else on hand''. %Etymology makes clear that
Different languages make different lump/split decisions in the conceptual
space, so much so that translational relatedness can, to a remarkable extent,
be used to recover the universal clustering \citep{Youn:2016}.
\Cref{sec:bground} by Veronika Lipp offers a more detailed background.

\subsection{Lexicographic Background \\ (Veronika Lipp)}

\label{sec:bground}

Lexical ambiguity is linguistically subdivided into two main categories:
\emph{homonymy and polysemy} \citep{Cruse:2004}. Homonymous words have
semantically unrelated and mutually incompatible meanings, such as
\emph{punch$_1$} , which means `a blow with a fist', and \emph{punch$_2$},
which means `a drink'. Some have described such homonymous word meanings as
essentially distinct words that accidentally have the same phonology (e.g.,
Murphy, 2002). Polysemous words, on the other hand, have semantically related
or overlapping senses (\cite{Cruse:2004,Jackendoff:2002, Pustejovsky:1995}),
such as \emph{mouth} meaning both `organ of body' and `entrance of cave'.

Two criteria have been proposed for the distinction between homonymy and
polysemy. The first criterion has to do with the \emph{etymological} derivation
of words. Words that are historically derived from distinct lexical items are
taken to be homonymous. However, the etymological criterion is not always
decisive. One reason is that there are many words whose historical derivation
is uncertain. Another reason is that it is not always very clear how far back
we should go in tracing the history of words \citep{Lyons:1977}.

The second criterion for the distinction between homonymy and polysemy has to
do with the \emph{relatedness/unrelatedness of meaning}.  The distinction
between homonymy and polysemy seems to correlate with the native speaker’s
feeling that certain meanings are connected and that others are not. Generally,
unrelatedness in meaning points to homonymy, whereas relatedness in meaning
points to polysemy.  However, in a large number of cases, there does not seem
to be an agreement among native speakers as to whether the meanings of the
words are related. So, it seems that there is not a clear dichotomy between
homonymy and polysemy, but rather a continuum from ‘‘pure’’ homonymy to
‘‘pure’’ polysemy \citep{Lyons:1977}.

Most discussions about lexical ambiguity, within theoretical and computational
linguistics, concentrate on polysemy, which can be further divided into two
types (c.f. \cite{Apresjan:1974,Pustejovsky:1995}). The first type of polysemy
is motivated by \emph{metaphor (irregular polysemy)}. In metaphorical polysemy,
a relation of analogy is assumed to hold between the senses of the word. The
basic sense of metaphorical polysemy is literal, whereas its secondary sense is
figurative. For example, the ambiguous word \emph{eye} has the literal basic
sense  `organ of the body' and the figurative secondary sense `hole in a
needle.' The other type of polysemy is motivated by \emph{metonymy (regular
polysemy)}. In metonymy, the relation that is assumed to hold between the
senses of the word is that of contiguity or connectedness.
% Apresjan argued that metonymically motivated polysemy respects the usual
% notion of polysemy, which is the ability of a word to have several distinct
% but related meanings.
In metonymic polysemy, both the basic and the secondary senses are literal. For
example, the ambiguous word \emph{chicken} has the literal basic sense
referring to the animal and the literal secondary sense of the meat of that
animal.

% regular or logical polysemy, and irregular or accidental polysemy

% ``After centuries of practical lexicography, there is still hardly any
% consensus on how to divide the semantic space of a lexical item''
% \citep{Meer:2006}, how many senses a word has, and in which order they should
% be presented.  Senses of polysemous words can be described at different levels
% of granularity, either more towards broader categories (lumping) or
% fine-grained ones (splitting), depending on the purpose of the dictionary, its
% size, target users etc.  Dictionaries, even those for the same types of users
% and of similar size, offer (very) different sense division of many words.
%
% While homonyms with their non-overlapping semantic fields easily lend
% themselves to \emph{sense-tagging}, polysemic meanings pose real challenge.
% Lexicons may comprise overlapping meanings, which render the unique assignment
% of the right meaning to the given word impossible even for human annotators.

% Collocations, semantic associations and colligations of a word play a
% decisive role in differentiating between its senses (lexical priming,
% \cite{Hoey:2005}).

% \citet{Youn:2016} analyses the universal structure of lexical meaning, and
% show that some concepts are more prone to polysemy than others, and that
% there are clusters of concepts within which polysemy is significantly more
% frequent than across the clusters.

\section{Multi-sense word embeddings}

Vector-space language models with more vectors for each meaning of a
word originate with \cite{Reisinger:2010}.
\cite{Huang:2012} trained the first neural-network-based MSE.
Both works use a uniform number of clusters for all words that they select
before training as potentially ambiguous.
The first system with adaptive sense numbers and an effective open-source
implementation is the 
modification of skip-gram \cite{Mikolov:2013d}, multi-sense skip-gram by
\cite{Neelakantan:2014}, where new senses are introduced during training by
thresholding the similarity of the present context to earlier contexts.

% \cite{Tian:2014}, Guo et al., 2014, Wu and Giles, 2015, Liu et al., 2015, Vu
% and Parker 2016, Šuster et al., 2016

% \citep{Neelakantan:2015, Bartunov:2015} improve upon the predecessors in
% three aspects: they do word sense discrimination simultaneously with the
% training of word embeddings, they determine the number of meanings based on
% the similarities of exemplars to global vectors, and make the system
% computationally more efficient.

\todo{Bayesian leaning:
\url{http://nostalgebraist.tumblr.com/post/161645122124/bayes-a-kinda-sorta-masterpost}}

\cite{Bartunov:2015,Li:2015} improve upon the heuristic thresholding by
formulating text generation as a Dirichlet process. In \adagram
\citep{Bartunov:2015}, senses may be merged as well as allocated during
training. \mutli-sense skip-gram \citep{Li:2015} applies the Chinese restaurant
process formalization of the Dirichlet process\footnote{Note the metathesis in
the name of the repo which is the only way of distinguishing it from the other
two multi-sense skip-gram models.}. Both \adagram and \mutli have a parameter for
semantics resolution (number of senses) $\alpha$ and $\gamma$ respectively. 

% https://sites.google.com/site/senseworkshop2017/

% Though sense representations have been already used for several NLP tasks
% such as text classification (Li and Jurafsky, 2015, Flekova and Gurevych,
% 2016) and knowledge base construction (Delli Bovi et al. 2015, Espinosa-Anke
% et al. 2016) and completion (Bordes et al. 2013, Wang et al. 2014)}, 
MSEs are yet in
research phase: \cite{Li:2015}  demonstrate that, when meta-parameters are
carefully controlled for, MSEs introduce a slight performance boost in
semantics-related tasks (semantic similarity for words and sentences, semantic
relation identification, part-of-speech tagging), but similar improvements can
be achieved by simply increasing the dimension of a single-sense embedding.

\section{Linear translation from MSEs}

 \cite{Mikolov:2013x} discovered that embeddings of different languages are so
 similar that a linear transformation can map vectors of the source language
 words to the vectors of their translations.

The method uses a seed dictionary of a few thousand words to learn translation
as a linear mapping $W: \mathbb{R}^{d_1}\rightarrow \mathbb{R}^{d_2}$ from the
source (monolingual) embedding to the target: the translation $z_i \in \Rt$ of
a source word $x_i \in \Ro$ is approximately its image $Wx_i$ by the mapping.
The translation model is trained with linear regression on the seed dictionary

\[\min_W \sum_i || Wx_i - z_i ||^2 \]

\noindent and can be used to collect translations for the whole vocabulary by
choosing $z_i$ to be the nearest neighbor (NN) of $Wx_i$.
We follow \cite{Mikolov:2013x} in using different metrics, Euclidean
 distance in training and cosine similarity in collection of translations.
% Though this choice is theoretically unmotivated, it seems to work better than
% more consistent use of metrics; but see \citep{Xing:2015} for opposing
% results.


In a multi-sense embedding scenario, \cite{Borbely:2016} take an MSE as the
source model, and a single-sense embedding as target model.  The quality of the
translation has been measured by training on the most frequent 5k word pairs
and evaluating on another 1k seed pairs.

% \subsection{Orthogonal restriction}

\todo{\cite{Artetxe:2016}}

% Besides optimising the transformation $W$ among all linear mappings, we
% followed \cite{Xing:2015} in restricting the approximation to orthogonal
% transformations (``rotations''). After computing the singular value
% decomposition
% 
% \[U\Sigma V=S_t^\top T_t\]
% 
% \noindent where $S_t$ and $T_t$ are the matrices consisting of the embedding vectors of
% the trainig words pairs in the source and the target space respectively, we
% took
% 
% \[W=U\mathbf{1}V\]
% 
% \noindent where $\mathbf 1$ is the rectangular identity matrix of appropriate shape.
% Similarly to \cite{Xing:2015}, we found that the orthogonal restriction yields
% significantly better results than general linear mapping.

\subsection{Reverse nearest neighbor search}

\todo{other than NLP, non-mapped points, Radovanović, Tomasev}

% First, worse method by Dinu:2015: they sample additional vectors in the
% source domain called \emph{pivots} besides the test words, and normalize the
% vector of similarities of each target item to the mapped pivots to length 1. 

A common error in linear translation is when there are \emph{hubs}, target
words returned as the translation of many words, which is wrong in most of the
cases.  In natural language processing, \cite{Dinu:2015} attack the problem
explicitly with a method they call \emph{global correction}.  Here, instead of
the original NN that we will call \emph{forward} NN search to contrast with the
more sophisticated method, they first rank source words by their similarity to
target words. In \emph{reverse} NN, source words are translated to the target
words to which they have the lowest (forward) NN rank.  If more target
words have the same forward rank, the decision is based on cosine similarity.
\todo{This tie braking is yet not included in our implementation.}


% We found that in reverse NN search it is important to resrict the vocabulary
% to the some tens of thousands most frequent words not only for memory saving
% (the $|V_{sr}|\times|V_{tg}|$ similarity matrix has to be sorted column-wise
% for forward and row-wise for reverse ranking, so at some point of the
% computation we keep the whole integer matrix of forward NN ranks in memory),
% but we found that a vocabulary cutoff of $2^{15}=32768$ both on the source
% and the target size yields better results than the more ambitious
% $2^{16}=65536$. This is not the case for forward NN seach, where accuracy
% increases with vocabulary limit, see \cref{tab:prec}.

\section{Experiments}

\subsection{Data}

We trained \neelakantan, \adagram~and \mutli~models on (original and stemmed
forms of) two Hungarian semi-gigaword corpora (.7--.8 B words), the Hungarian
Wecorpus (Webkorpusz, \cite{Halacsy:2004}) and (the non-social-media part of) the
Hungarian National Corpus (HNC, \cite{Oravecz:2014}).  We used Wiktionary as our
seed dictionary, extracted with
\texttt{wikt2dict}\footnote{\url{https://github.com/juditacs/wikt2dict}}
\citep{Acs:2013}. We tried several English embeddings as target, including the
300 dimensional skip-gram with negative sampling model
\texttt{GoogleNews} released with \texttt{word2vec}
\citep{Mikolov:2013f}\footnote{\url{https://code.google.com/archive/p/word2vec/}},
and those released with GloVe
\citep{Pennington:2014}\footnote{\url{https://nlp.stanford.edu/projects/glove/}}.

\subsection{Results}

\newcommand{\any}{\texttt{any}}
\newcommand{\disamb}{\texttt{disamb}}

We evaluate MSE models in two ways referred as \any~and \disamb.  The method
\any~has been used for tuning the (meta)parameters of both the source and the
target embedding: a traditional, single-sense translation has been trained
between the first \todo{all} sense vector of each word form and its
translations. (If the training word is ambiguous in the seed dictionary, all
translations have been included in the training data.)  Exploiting the multiple
sense vectors, one word can have more than one translation.  During test, a
source word was accepted if \any~\todo{the 1st}of its sense vectors had at
least one good translation among the ten NNs. \Cref{tab:prec} shows results by
the best models.

\begin{table*} 
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{lrlrr|rrrrrr|rrrrrr}
      \toprule
        & dim & $\alpha/\gamma$ & $p$ & $m$  & \multicolumn{6}{c|}{forward NN search} & \multicolumn{6}{c}{reverse NN search} \\
      vocab cutoff &&&&& \multicolumn{2}{c}{8192} & \multicolumn{2}{c}{16384}
      & \multicolumn{2}{c|}{32768} & \multicolumn{2}{c}{8192} &
      \multicolumn{2}{c}{16384} & \multicolumn{2}{c}{32768} \\
      \midrule
      HNC	        & 800	& .02	&       & 100   & 24.8\%	&  0	& 29.9\%	&  0	& 30.3\%	&  0	& 47.6\%	&  65	  & 47.8\%	&  57	  & 54.2\%	&  77   \\
      HNC stemmed	& 800	& .05	&       & 385 K & 24.0\%	&  2	& 29.2\%	&  2	& 27.8\%	&  0	& 55.1\%	&  102	& 54.5\%	&  94	  & 59.5\%	&  113  \\
      \neelakantan Webkorpusz&300&--&2   &14665  & 9.1\%   &  3  & 10.9\%  &  4 & 10.7\%   &  3  & 52.4\%  &  63   & 56.6\%  &  110  & 60.6\%  &  151  \\
      %\mutli Webkorp\footnotemark  &&&&    & 1.0\% &  0 & 1.5\% &  0 & 1.6\% &  0 & 42.9\% &  36 & 48.7\% &  81 & 63.4\% &  169 \\
      \mutli Webkorpusz &300&.25 &       & 71    & 24.6\% &  0 & 30.2\% &  2 & 30.6\% &  2 & 49.7\% &  46 & 54.1\% &  96 & 64.5\% &  163 \\ 
      HNC         & 160 & .05 & 3     & 200   & 25.0\% &  4 & 33.6\% &  4 & 34.6\% &  7 & 52.0\% &  85 & 51.8\% &  85 & 65.2\% &  155 \\ 
      Webkorpusz	    & 800	& .05	&       & 100	  & 29.0\%	&  7	& 36.4\% &  10	& 36.6\%	&  11	& {\bf 58.0\%}	&  100	& {\bf 58.3\%}	&  110	& 68.6\%	&  179  \\
      HNC	        & 600	& .1  & 3     & 50	  & 30.4\%	&  2	& 39.6\% &  6	& {\bf 41.7\%}	&  11	& 55.7\%	&  67	  & 56.6\%	&  97	  & 70.9\%	&  175  \\
      HNC	        & 600	& .05	& 5     & 100	  & {\bf 31.5\%}	&  4	& {\bf 40.3\%}	&  5	& 40.5\%	&  7	& 55.0\%	&  96	  & 55.0\%	&  101	& 70.0\%	&  164  \\
      Webkorpusz	    & 800	& .1  &       & 100	  & 28.7\%	&  10	& 36.2\% &  9	& 36.7\%	&  16	& 53.8\%	&  48	  & {\bf 58.3\%}	&  99	  & {\bf 74.3\%}	&  229  \\
      \bottomrule
    \end{tabular}
  }
  \caption{Precision of \any~forward and reverse NN and the number of word
  forms with non-synonymic vectors (\disamb).  The source embedding has been
  trained with \adagram, except for when indicated otherwise (\neelakantan,
  \mutli).  The meta-parameters are $d$imension, the resolution parameter
  ($\alpha$ in \adagram and $\gamma$ in \mutli), the maximum number of
  $p$rototypes (sense vectors) and the vocabulary cutoff ($m$in-freq). The two
  models with large $m$s have technically no cut-off.} 
    \label{tab:prec}
\end{table*}

In \disamb, we used the same translation matrix as in \any, and inspected the
translations of the different sense vectors to see whether the vectors really
model different senses (not synonyms).
% We separated \emph{common} translations of word vectors, i.e.~translations
% $t$ of a source word such that all the source sense vectors that have at
% least one good translation have $t$ among their translations.  Common
% translations do not evidence different senses. Instead 
The lowest requirement for sense vectors $s_1, s_2$ for not being synonyms is
that the sets of corresponding NN@$k$ hits are different. This is satisfied
only in \todo{\%} of the words with more than one sense vector.

\subsection{Qualitative analysis}

We analyzed qualitatively the successfully disambiguated words, sorting them by
the cosine similarity $s$ of translations of different sense vectors. We found
that most of the few cases when there are more than two sense vectors with a
good NN@1 translation are due to that the seed dictionary contains some
non-basic translation e.g. \emph{kapcsolat} `relationship, conjunction' has
`affair' among its seed translations. In these cases, we two sense vectors
arbitraryly. \todo{When there are sense vectors with more than two NN@$k$ hits
, the choice of the corresponding target words is arbitrary.} \Cref{tab:alkoto}
shows words with $s<.3$, most with sense vectors corresponding to senses with
clear difference.  This is similar to clustering the sense vectors for each
word, but here we restrict our analysis to sense vectors that prove to be
sensible in linear translation.

\begin{table*}
  \begin{tabular}{cllc}
  0.0182 & alkotó 	& constituent, creator, composer, creative	&  1.0 \\
  0.0407 & rész 	& parcel, episode	&  1.0 \\
  0.0447 & ér 	& vein, arrive	&  0.66 \\
  0.0509 & előzetes 	& preliminary, trailer	&  1.0 \\
  0.0543 & villamos 	& electro, tram	&  0.5 \\
  0.0555 & nemes 	& peer, sublime, noble	&  1.0 \\
  0.0654 & sorozat 	& succession, serial, suite	&  1.0 \\
  0.1005 & egyenes 	& linear, frank, straightforward	&  1.0 \\
  0.1007 & kar 	& choir, arm	&  1.0 \\
  0.1153 & test 	& hull, corpus	&  1.0 \\
  0.1155 & rendelet 	& doom, regulation	&  0.66 \\
  0.1267 & tömeg 	& mob/crowd, bulk, cloud, mob, crowd	&  1.0 \\
  0.1410 & terem 	& classroom, yield, hall	&  1.0 \\
  0.1512 & hátsó 	& hinder, rear	&  1.0 \\
  0.1587 & szem 	& grain, optic	&  1.0 \\
  0.1760 & hatalom 	& empire, leverage	&  0.33 \\
  0.1809 & megfelel 	& suit, correspond	&  1.0 \\
  0.1848 & Judit 	& Jude, Jodi	&  0.4 \\
  0.1872 & induló 	& march, candidate	&  1.0 \\
  0.1920 & cigány 	& Roma, Rom	&  0.4 \\
  0.1926 & szakértő 	& judge, adept, adviser	& 0.42 \\
  0.1933 & eltérés 	& departure, variance	&  0.4 \\
  0.2017 & sor 	& queue/row, rank	&  1.0 \\
  0.2028 & piros 	& rot, hearts	&  1.0 \\
  0.2036 & felső 	& superior, overhead	&  1.0 \\
    0.2053 & megelőző 	& preventive/preventative, preceding,
    \\&&\hspace{1cm}preventative, foregoing, anterior	&  0.71 \\
  0.2202 & fekvő 	& landscape, lying	&  0.66 \\
  0.2235 & lengyel 	& Pole, Polish	&  1.0 \\
  0.2236 & környező 	& ambient, surrounding	& 0.66 \\
  0.2345 & ok 	& motive, okay	&  1.0 \\
  0.2424 & kiegészítő 	& complementary, expansion, supplementary, accessory	&  0.8 \\
  0.2428 & készít 	& manufacture, prepare	&  1.0 \\
  0.2436 & vár 	& castle/fortress, await, castle	&  1.0 \\
  0.2514 & előbbi 	& preceding, anterior	&  0.66 \\
  0.2558 & kötelezettség 	& engagement, obligation	& 0.66 \\
  0.2569 & némi 	& generic, genital	&  0.5 \\
  0.2575 & szín 	& suit, shed	&  0.66 \\
  0.2616 & vállalkozó 	& contractor, entrepreneur	&  1.0 \\
  0.2639 & közepes 	& mediocre, moderate	&  0.66 \\
  0.2732 & követ 	& haunt, succeed	&  0.66 \\
  0.2818 & puszta 	& pure, desert, bleak, bare, sheer	&  0.55 \\
  0.2838 & bemutató 	& premiere, presenter, exhibition	& 1.0 \\
  0.2946 & előre 	& onward, beforehand	&  0.0 \\
\end{tabular}
\caption{Sense vectors with rather different translations. Alternatives like
  mob/crowd denote more good non-common translations}
\label{tab:alkoto}
\end{table*}

The clearest case of homonymy is when senses belong to different POS, and the
translations reflect these POSs, e.g.~\emph{nő} `woman; increase' or \emph{vár}
\todo{update examples} `wait; case'.  We note that some POSs in Hungarian have
blurred borders, e.g.  it is debatable whether the nominal \emph{önkéntes}
`voluntary; volunteer' is ambiguous. 

\todo{Note that in the \fl~approach, POS-difference alone is not enough for
analyzing a word as ambiguous, e.g.~the only difference between the senses of
\emph{alkalmazott}, `employee; applied' is that employment is the application
of people for work; in the case of \emph{belső} `internal; interior', the noun
\emph{interior} refers to the \emph{internal} part of a building.} 

% More interesting are words whose one sense is a special case of an other
% sense of the same word, e.g.~ \emph{cikk}, `item; article' (an article is an
% item in a newspaper); \emph{eredmény}, `score; result' (a score is a result
% measured by a number); \emph{magas}, `tall; high' (tall is used for people in
% spite of high).  The meanings of \emph{idegen}, `strange, alien; foreign' are
% special cases of unfamiliar (person versus language).

% Finally we mention two cases where the relation between the two senses is
% more idiosyncratic, but in a monosemic approach, they will have a single
% representation: \emph{beteg} means `ill, sick; patient'. Though \emph{ill} is
% a health state and \emph{patient} is a situational role, the fact that
% patients of doctors are usually ill, may suffice for identifying the `patent'
% based on the representation of \emph{ill}; and a monosemic system is designed
% to give account of metaphorical relations like the one between the meanings
% of \emph{világos}, `bright; clear' as well.

\todo{missing sense \\ édes [['sweetheart'], ['cute \\
  közvetlen [['informal'], ['casual}

% If the distribution turns out to be multi-modal (i.e.~some words have
% stipulated meanings with similar translations and others have senses with
% greater distance, but intermediate distances are less frequent), we can
% separate synonyms from true ambiguity. Then we say that the MSE discovered a
% real ambiguous word, if two sense vectors have such translations that are
% licensed by the gold dictionary, and the distance of these words in the
% target space is above the threshold judged significant by the analysis of the
% distance distribution.

When a word has more than two sense vectors with good non-common translations,
with most of the words there is a more ``general'' vector (or two, in the case
of \emph{tud}) that has more good translations, and good translations of the
other vectors are mostly among the good translations of this general vector,
see \cref{tab:like_than}.

\begin{table*}[t]
  %\footnotesize
  \begin{tabular}{ll}
    \toprule
    mint	& \{like, than, as\}, \{like\}, \{than\}	\\
    majd	& \{then, later\}, \{then\}, \{later\}	\\
    talán	& \{probably, perhaps\}, \{probably, maybe\}, \{perhaps\}	\\
    tud	& \{understand, can\}, \{know, understand\}, \{know\}, \{can\}	\\
    program	& \{project, programme, program\}, \{project, programme\}, \{project, program\}	\\
    terület	& \{region, area, territory\}, \{region, district\}, \{region, area\}, \{area\}	\\
    felső	& \{upper, top\}, \{upper\}, \{higher\}	\\
    díj	& \{charge, fee, rate\}, \{fee\}, \{award\}	\\
    kicsi	& \{little, small\}, \{little\}, \{small\}	\\
    erő	& \{strength, power, force, stress, energy\}, \{strength, power\}, \{force\}	\\
    lakás	& \{apartment, housing\}, \{residence, housing\}, \{rooms, apartment\}	\\
    kérelem	& \{application, appeal\}, \{application\}, \{appeal\}	\\
    figyel	& \{watch, listen\}, \{watch\}, \{listen\}	\\
    \bottomrule
  \end{tabular}
  \caption{Words with at least three good sense vectors (forward NNs, source is
  the AdaGram model trained on HNC (600 dimensions, $\alpha=.05$)}
  \label{tab:like_than}
\end{table*}

\section{Acknowledgments}

1957 was an influential year in linguistics: \cite{Harris:1957} developed the
frequency-aware variant of the distributional method, \cite{Osgood:1957}
pioneered vector space models, and the author of a more recent conceptual
meaning representation framework \citep{Kornai:2010,Kornai:2017} was born.
Fifty years later (more precisely in fall 2006) I learned András during a class
he taught on the book he was writing \citep{Kornai:2007}. I heard about
\emph{deep cases} and \emph{k\={a}rakas} sooner than I did about \emph{thematic
roles}. The fact that he taught me computational linguistics in a master and
disciple fashion comes at no surprise as there have been no such PhD school in
Hungary, not to mention mathematical linguistics proper.
% He advised my so called \emph{topic labor} and MSc at the Budapest University
% of Technology and Economics, and
I started my PhD as a member of the 4lang project at the Institute for Computer
Science and Control of the Hungarian Academy of Sciences.

Laozi says that a good leader does not leave a footprint, and András encouraged
us to be independent and effective. One of his remarkable citations is that
``It's easier to ask forgiveness than it is to get permission''. The proverb is
sometimes attributed to the Jesuit, who are similar to András in having had a
great impact on what I've become in the past ten years. The real source of the
proverb is Grace Hopper, a computer scientist and US navy admiral who invented
the first compiler and popularized the idea of machine-independent programming
languages. Both the alleged and the real source remind me to that I still have
lessons to learn in efficiency.

\todo{Thank you}

\smallskip

\Cref{sec:bground} was written by Veronika Lipp (Research Institute for
Linguistics of the Hungarian Academy of Sciences). The orthogonal approximation
was implemented following a
code\footnote{\url{https://github.com/hlt-bme-hu/eval-embed}} by Gábor Borbely.

\bibliographystyle{acl_natbib}
\bibliography{ml}

\end{document}

\footnotetext{Both the original skip-gram model and its multi-sense variant
have two kind of vector for each word, the vanilla ``word'' vector, and a
context vector. The first \mutli model denotes context vectors.  otherwise we
used word vectors.}
